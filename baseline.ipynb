{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = 'Trump Rally Speeches/'\n",
    "files = os.listdir(path)\n",
    "files = [path + file for file in files]\n",
    " \n",
    "dates = []\n",
    "locations = []\n",
    "years = []\n",
    "days = []\n",
    "months = []\n",
    "speeches_text = []\n",
    " \n",
    "month_ab = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep','Oct', 'Nov', 'Dec']\n",
    "\n",
    "for file in files:\n",
    "    for month in month_ab:\n",
    "        if month in file:\n",
    "            locations.append(file[file.find('/')+1:file.find(month)])\n",
    "            break\n",
    "    for i, mont in enumerate(month_ab):\n",
    "        if month in file:\n",
    "            date = file[file.find(month):file.find('.txt')]\n",
    "            dates.append(date)\n",
    "            months.append(date[:3])\n",
    "            days.append(str(date[3]))\n",
    "            years.append(date[-4:])\n",
    "            break   \n",
    "        \n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        speeches_text.append(f.read())     \n",
    "        \n",
    "import pandas as pd\n",
    " \n",
    "df = pd.DataFrame({'Speech':files, 'Date':dates, 'Location':locations, 'Year':years, 'Month':months, 'Day':days, 'Speech_Text':speeches_text})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thank thank thank vice president pence hes good guy weve done great job together merry christmas mic\n",
      "Thank you. Thank you. Thank you to Vice President Pence. He's a good guy. We've done a great job tog\n",
      "thank thank thank vice president pence hes good guy weve done great job together merry christmas mic\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import preprocessing_pipline\n",
    "\n",
    "preprocessing = preprocessing_pipline(df['Speech_Text'])\n",
    "df['Speech_Text_prepro'] = preprocessing.preprocess_light()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So for the baseline model, we want something simple that can be used as a benchmark for the other models. We will use N-gram that will operate on the tokenized text. \n",
    "\n",
    "Explaination about N-gram: \n",
    "\n",
    "- \"N\" in N-gram means the number of words that will be used as a feature. With N = 3 we will we will use 3 words as a feature. \n",
    "- It is based on the frequency of words sequence. For exemple with N = 2 we will count the frequency of each pair consecutive words.\n",
    "- Then we can we can calculate the probability of the next word given the N-1(or more) previous words.\n",
    "-> Choose from the best probability\n",
    "\n",
    "These leads to a low understanding of the context and long term dependencies. However this is what we are looking for in this baseline model.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://i.stack.imgur.com/8ARA1.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We dont have x and y like in a classification problem. We have a sequence of words and we want to predict the next word. So we will use a sliding window to create the features and the labels. We so create a kind of y for each x, so we need to split between test and train to have an realist evaluation, and see hox it generalize.\n",
    "\n",
    "We can not realy use accuracy as a metric cause it's not the prediction of a class. So we will use the perplexity:\n",
    "The lower the perplexity is the better the model is!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['go', 'going', 'alabama', 'privilege', 'say', 'joe', 'understand', 'soo', 'special', 'every']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "text_corpus = [word for speech in df['Speech_Text'].str.split() for word in speech]\n",
    "train_corpus, test_corpus = train_test_split(text_corpus, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocess text \n",
    "text_corpus_prepro = [word for speech in df['Speech_Text_prepro'].str.split() for word in speech]\n",
    "train_corpus_prepro, test_corpus_prepro = train_test_split(text_corpus_prepro, test_size=0.2, random_state=42)\n",
    "# some preprocessed text;\n",
    "print(train_corpus_prepro[:10])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We create a class \"my_model_NGram\" that will be used to train and test the model. \n",
    "It will permit to :\n",
    "- Build the model -> construct a dictionary of N-gram, where each key represent a sequence of N-1 words and the value is a list of possible next words with their frequency.\n",
    "- Then I ve found a way to calculate the performance of the model -> calculate the perplexity of the model on the test set.\n",
    "  We calculate the likelihood of the test set given the model(trained). Then we calculate the perplexity with the formula: 2^(-1/N * log(likelihood))\n",
    "- Finaly we can try to generate text from the start of a sentence. We will use the trained model to predict the next word and then we will add this to the sentence and so on. We can choose the number of words we want to generate. Also I'va create a version with random choice of the next word, this is not completly random cause we still use the frequency of... but it create interesting results :)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Text: I want to and It's over wanted of have all, millions job, see again. wage with toward the just take they killer. AIDS\n",
      "Perplexity: 6204.833172882722\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "from nltk import ngrams\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from math import log\n",
    "\n",
    "\n",
    "class my_model_NGram:\n",
    "    def __init__(self, n, text_corpus):\n",
    "        self.n = n\n",
    "        self.text_corpus = text_corpus\n",
    "        self.ngrams = list(ngrams(text_corpus, n))\n",
    "        self.model = self.build_model()\n",
    "\n",
    "    def build_model(self):\n",
    "        model = {}\n",
    "        for ngram in self.ngrams:\n",
    "            prefix = tuple(ngram[:-1])\n",
    "            target = ngram[-1]\n",
    "\n",
    "            if prefix in model:\n",
    "                model[prefix].append(target)\n",
    "            else:\n",
    "                model[prefix] = [target]\n",
    "        return model\n",
    "\n",
    "    def generate_text_with_random(self, seed_text, max_length=100):\n",
    "        output_text = seed_text.split()\n",
    "        prefix = tuple(output_text[-(self.n - 1):])\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            if prefix not in self.model:\n",
    "                break\n",
    "            next_word = random.choice(self.model[prefix])\n",
    "            output_text.append(next_word)\n",
    "            prefix = prefix[1:] + (next_word,)\n",
    "\n",
    "        return ' '.join(output_text)\n",
    "    \n",
    "    def generate_text(self, seed_text, max_length=100):\n",
    "        output_text = seed_text.split()\n",
    "        prefix = tuple(output_text[-(self.n - 1):])\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            if prefix not in self.model:\n",
    "                break\n",
    "            next_word = self.model[prefix][0]\n",
    "            output_text.append(next_word)\n",
    "            prefix = prefix[1:] + (next_word,)\n",
    "\n",
    "        return ' '.join(output_text)\n",
    "\n",
    "    def calculate_perplexity(self, test_corpus):\n",
    "        test_ngrams = list(ngrams(test_corpus, self.n))\n",
    "        log_prob_sum = 0\n",
    "        num_ngrams = len(test_ngrams)\n",
    "\n",
    "        for ngram in test_ngrams:\n",
    "            context = tuple(ngram[:-1])\n",
    "            word = ngram[-1]\n",
    "\n",
    "            if context in self.model:\n",
    "                word_probabilities = self.model[context]\n",
    "                if word in word_probabilities:\n",
    "                    word_probability = (word_probabilities.count(word) + 1) / (len(word_probabilities) + len(self.text_corpus))\n",
    "                else:\n",
    "                    word_probability = 1 / (len(word_probabilities) + len(self.text_corpus))\n",
    "            else:\n",
    "                word_probability = 1 / len(self.text_corpus)\n",
    "\n",
    "            log_prob_sum += log(word_probability)\n",
    "\n",
    "        perplexity = 2 ** (-log_prob_sum / num_ngrams)\n",
    "        return perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 2 Perplexity: 3380.9841212515016\n",
      "N = 3 Perplexity: 6099.203647607046\n",
      "N = 4 Perplexity: 6204.833172882722\n",
      "N = 5 Perplexity: 6205.798200535082\n"
     ]
    }
   ],
   "source": [
    "# try different N value\n",
    "for n in range(2, 6):\n",
    "    ngram_model = my_model_NGram(n=n, text_corpus=train_corpus)\n",
    "    perplexity = ngram_model.calculate_perplexity(test_corpus)\n",
    "    print(\"N =\", n, \"Perplexity:\", perplexity)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "N = 2 Perplexity: 3146.029000302403\n",
      "N = 3 Perplexity: 3886.032907464139\n",
      "N = 4 Perplexity: 3889.3496831546913\n",
      "N = 5 Perplexity: 3889.3496505377457\n"
     ]
    }
   ],
   "source": [
    "# with preprocessing\n",
    "for n in range(2, 6):\n",
    "    ngram_model = my_model_NGram(n=n, text_corpus=train_corpus_prepro)\n",
    "    perplexity = ngram_model.calculate_perplexity(test_corpus_prepro)\n",
    "    print(\"N =\", n, \"Perplexity:\", perplexity)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our best perplexity 3380 with N = 2 without preprocessing\n",
    "And with preprocessing we have 3146 with N = 2 and better value for any other N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed text: I want to\n",
      "Generated Text: I want to lying people supporters to give they'd crowd. all did at But share today, party President tonight, we because of The\n",
      "Seed text: I will do\n",
      "Generated Text: I will do with there all loves them, is a paying their It's we very the that. an up. job, all true. me\n"
     ]
    }
   ],
   "source": [
    "# try different seed no random\n",
    "seeds = [\"I want to\", \"I will do\"]\n",
    "ngram_model = my_model_NGram(n=3, text_corpus=train_corpus)\n",
    "for seed in seeds:\n",
    "    generated_text = ngram_model.generate_text(seed, max_length=20)\n",
    "    print(\"Seed text:\", seed)\n",
    "    print(\"Generated Text:\", generated_text)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Seed text: I want to\n",
      "Generated Text: I want to that's us Ohio, can called a kept again. And and war. 90. wonderful actually renegotiated just this said, She were\n",
      "Seed text: I will do\n",
      "Generated Text: I will do on be for the we've deals adding announced the you you And be we He's he They're study will who\n"
     ]
    }
   ],
   "source": [
    "# with random\n",
    "ngram_model = my_model_NGram(n=3, text_corpus=train_corpus)\n",
    "for seed in seeds:\n",
    "    generated_text = ngram_model.generate_text_with_random(seed, max_length=20)\n",
    "    print(\"Seed text:\", seed)\n",
    "    print(\"Generated Text:\", generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bc11ff26b6a5fa7a865d8250cd70cfc7d3342e295774ddad3442a38e57bd6f20"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
