{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM for Trump text generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://miro.medium.com/v2/resize:fit:984/1*Mb_L_slY9rjMr8-IADHvwg.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM stands for Long Short-Term Memory, and it's a type of RNN that is known to be effective for text generation, which is our primary goal.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- Capable of capturing complex sequential dependencies.\n",
    "- Can retain context and remember information from the entire sequence, making it ideal for contextual situations, such as discussing Trump.\n",
    "\n",
    "Limits:\n",
    "\n",
    "- LSTM models require a large amount of data; it's a complex model that demands extensive training to handle context effectively.\n",
    "- LSTMs are computationally intensive, and tuning hyperparameters has been a real challenge during this project."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow will be used for the integration of the LSTM model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = 'Trump Rally Speeches/'\n",
    "files = os.listdir(path)\n",
    "files = [path + file for file in files]\n",
    " \n",
    "dates = []\n",
    "locations = []\n",
    "years = []\n",
    "days = []\n",
    "months = []\n",
    "speeches_text = []\n",
    " \n",
    "month_ab = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep','Oct', 'Nov', 'Dec']\n",
    "\n",
    "for file in files:\n",
    "    for month in month_ab:\n",
    "        if month in file:\n",
    "            locations.append(file[file.find('/')+1:file.find(month)])\n",
    "            break\n",
    "    for i, mont in enumerate(month_ab):\n",
    "        if month in file:\n",
    "            date = file[file.find(month):file.find('.txt')]\n",
    "            dates.append(date)\n",
    "            months.append(date[:3])\n",
    "            days.append(str(date[3]))\n",
    "            years.append(date[-4:])\n",
    "            break   \n",
    "        \n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        speeches_text.append(f.read())     \n",
    "        \n",
    "import pandas as pd\n",
    " \n",
    "df = pd.DataFrame({'Speech':files, 'Date':dates, 'Location':locations, 'Year':years, 'Month':months, 'Day':days, 'Speech_Text':speeches_text})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the two preprocessing function defined in preprocessing_pipline.\n",
    "One only remove the punctuation and do lower case, the other one also remove the stop and rare words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "thank thank thank vice president pence hes good guy weve done great job together merry christmas mic\n",
      "Thank you. Thank you. Thank you to Vice President Pence. He's a good guy. We've done a great job tog\n",
      "thank thank thank vice president pence hes good guy weve done great job together merry christmas mic\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import preprocessing_pipline\n",
    "\n",
    "preprocessing = preprocessing_pipline(df['Speech_Text'])\n",
    "df['Speech_Text_prepro'] = preprocessing.preprocess_light()\n",
    "df['Speech_Text_prepro2'] = preprocessing.preprocess()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data \n",
    "1. Without preprocessing -> terrible results... not intresting\n",
    "2. Light preprocessing \n",
    "3. Heavy preprocessing (stop words and rare words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "text_corpus = [word for speech in df['Speech_Text'].str.split() for word in speech]\n",
    "\n",
    "# Preprocess text \n",
    "text_corpus_prepro = [word for speech in df['Speech_Text_prepro'].str.split() for word in speech]\n",
    "\n",
    "text_corpus_prepro2 = [word for speech in df['Speech_Text_prepro2'].str.split() for word in speech]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess before using LSTM\n",
    "\n",
    "Input is the sequences and output the next word to guess.  \n",
    "-We tokenize them, and we applied a padding to both: make them the same lenght\n",
    "-Convert to numpy array  \n",
    "-Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-05 22:22:39.147370: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-05 22:22:39.182658: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-05 22:22:39.182696: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-05 22:22:39.182736: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-05 22:22:39.190621: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-05 22:22:39.191137: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-05 22:22:40.113116: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_for_LSTM(text_corpus, lenght_of_sequences=10, test_size=0.2):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(text_corpus)\n",
    "\n",
    "    input = []\n",
    "    output = []\n",
    "\n",
    "\n",
    "    for i in range(lenght_of_sequences, len(text_corpus)):\n",
    "        input.append(text_corpus[i - lenght_of_sequences:i])\n",
    "        output.append(text_corpus[i])\n",
    "\n",
    "    input = tokenizer.texts_to_sequences(input)\n",
    "    output = tokenizer.texts_to_sequences(output)\n",
    "\n",
    "    # padding\n",
    "    input = pad_sequences(input, maxlen=lenght_of_sequences, padding='pre')\n",
    "    output = pad_sequences(output, maxlen=1, padding='pre')\n",
    "    \n",
    "    # into numpy arrays\n",
    "    input = np.array(input)\n",
    "    output = np.array(output)\n",
    "\n",
    "    # Split your data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(input, output, test_size=test_size, random_state=42)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, tokenizer\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, tokenizer = preprocess_for_LSTM(text_corpus_prepro, lenght_of_sequences=10, test_size=0.2)\n",
    "X_train2, X_test2, y_train2, y_test2, tokenizer2 = preprocess_for_LSTM(text_corpus_prepro2, lenght_of_sequences=10, test_size=0.2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "class my_model_LSTM:\n",
    "    def __init__(self, n, num_unique_words, max_sequence_length):\n",
    "        \"\"\"\n",
    "        Initialize the LSTM model\n",
    "        \n",
    "        Args:\n",
    "            n (int): Number of LSTM units.\n",
    "            num_unique_words (int): Number of unique words \n",
    "            max_sequence_length (int): Maximum sequence length.\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.num_unique_words = num_unique_words\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.model = self.build_model()\n",
    "        \n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        Build and compile the LSTM model\n",
    "        \"\"\"\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(Embedding(self.num_unique_words, self.n))\n",
    "        model.add(LSTM(units=self.n))\n",
    "        model.add(Dense(units=self.num_unique_words, activation='softmax'))\n",
    "        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "     \n",
    "    def train(self, X_train, y_train, epochs=10):\n",
    "        self.model.fit(X_train, y_train, epochs=epochs)\n",
    "         \n",
    "    def predict(self, test_corpus):\n",
    "        return self.model.predict(test_corpus)\n",
    "     \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        return self.model.evaluate(X_test, y_test)\n",
    "     \n",
    "    def save(self, path):\n",
    "        self.model.save(path)\n",
    "         \n",
    "    def load(self, path):\n",
    "        self.model = tf.keras.models.load_model(path)\n",
    "         \n",
    "    def generate_text(self, seed_text, max_length=100):\n",
    "        \"\"\"\n",
    "        Generate text based on a seed text\n",
    "\n",
    "        Args:\n",
    "            seed_text (str): Initial text seed, we will generate what's next\n",
    "            max_length (int): Size of the text to generate\n",
    "\n",
    "        Returns:\n",
    "            str: Generated text\n",
    "        \"\"\"\n",
    "        output_text = seed_text.split()\n",
    "        prefix = output_text[-(self.n - 1):]\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            input = [tokenizer.word_index[word] for word in prefix]\n",
    "            input = tf.expand_dims(input, 0)\n",
    "\n",
    "            predictions = self.model(input)\n",
    "            predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "            if len(predictions.shape) == 1:\n",
    "                predictions = tf.expand_dims(predictions, 0)\n",
    "\n",
    "            predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "            next_word = tokenizer.index_word[predicted_id]\n",
    "\n",
    "            output_text.append(next_word)\n",
    "            prefix = prefix[1:] + [next_word]\n",
    "\n",
    "        return ' '.join(output_text)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "4720/4720 [==============================] - 39s 8ms/step - loss: 7.3394 - accuracy: 0.0138\n",
      "Epoch 2/40\n",
      "4720/4720 [==============================] - 38s 8ms/step - loss: 6.8464 - accuracy: 0.0362\n",
      "Epoch 3/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 6.4224 - accuracy: 0.0694\n",
      "Epoch 4/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 6.0620 - accuracy: 0.0997\n",
      "Epoch 5/40\n",
      "4720/4720 [==============================] - 38s 8ms/step - loss: 5.7583 - accuracy: 0.1255\n",
      "Epoch 6/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 5.5041 - accuracy: 0.1444\n",
      "Epoch 7/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 5.2868 - accuracy: 0.1613\n",
      "Epoch 8/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 5.0968 - accuracy: 0.1765\n",
      "Epoch 9/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 4.9282 - accuracy: 0.1907\n",
      "Epoch 10/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 4.7775 - accuracy: 0.2052\n",
      "Epoch 11/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 4.6424 - accuracy: 0.2178\n",
      "Epoch 12/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 4.5190 - accuracy: 0.2300\n",
      "Epoch 13/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 4.4079 - accuracy: 0.2407\n",
      "Epoch 14/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 4.3037 - accuracy: 0.2529\n",
      "Epoch 15/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 4.2096 - accuracy: 0.2637\n",
      "Epoch 16/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 4.1220 - accuracy: 0.2745\n",
      "Epoch 17/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 4.0401 - accuracy: 0.2841\n",
      "Epoch 18/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 3.9653 - accuracy: 0.2936\n",
      "Epoch 19/40\n",
      "4720/4720 [==============================] - 38s 8ms/step - loss: 3.8946 - accuracy: 0.3031\n",
      "Epoch 20/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 3.8289 - accuracy: 0.3116\n",
      "Epoch 21/40\n",
      "4720/4720 [==============================] - 38s 8ms/step - loss: 3.7669 - accuracy: 0.3193\n",
      "Epoch 22/40\n",
      "4720/4720 [==============================] - 38s 8ms/step - loss: 3.7091 - accuracy: 0.3264\n",
      "Epoch 23/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 3.6563 - accuracy: 0.3344\n",
      "Epoch 24/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 3.6051 - accuracy: 0.3416\n",
      "Epoch 25/40\n",
      "4720/4720 [==============================] - 38s 8ms/step - loss: 3.5580 - accuracy: 0.3491\n",
      "Epoch 26/40\n",
      "4720/4720 [==============================] - 38s 8ms/step - loss: 3.5114 - accuracy: 0.3552\n",
      "Epoch 27/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 3.4717 - accuracy: 0.3606\n",
      "Epoch 28/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 3.4324 - accuracy: 0.3656\n",
      "Epoch 29/40\n",
      "4720/4720 [==============================] - 38s 8ms/step - loss: 3.3942 - accuracy: 0.3709\n",
      "Epoch 30/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 3.3587 - accuracy: 0.3764\n",
      "Epoch 31/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 3.3271 - accuracy: 0.3806\n",
      "Epoch 32/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 3.2947 - accuracy: 0.3858\n",
      "Epoch 33/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 3.2656 - accuracy: 0.3908\n",
      "Epoch 34/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 3.2373 - accuracy: 0.3938\n",
      "Epoch 35/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 3.2106 - accuracy: 0.3982\n",
      "Epoch 36/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 3.1860 - accuracy: 0.4028\n",
      "Epoch 37/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 3.1599 - accuracy: 0.4071\n",
      "Epoch 38/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 3.1378 - accuracy: 0.4097\n",
      "Epoch 39/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 3.1186 - accuracy: 0.4122\n",
      "Epoch 40/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 3.0941 - accuracy: 0.4162\n"
     ]
    }
   ],
   "source": [
    "num_unique_words = len(tokenizer.word_index) + 1\n",
    "max_sequence_length = 40\n",
    "model_LSTM = my_model_LSTM(50, num_unique_words, max_sequence_length)\n",
    "history = model_LSTM.train(X_train, y_train, epochs=40)\n",
    "model_LSTM.save('model_LSTM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1180/1180 [==============================] - 4s 3ms/step - loss: 7.4937 - accuracy: 0.1292\n",
      "LSTM Model Accuracy: 0.12921422719955444\n",
      "LSTM Model Loss: 7.493661403656006\n"
     ]
    }
   ],
   "source": [
    "predictions = model_LSTM.evaluate(X_test, y_test)\n",
    "print(\"LSTM Model Accuracy:\", predictions[1])\n",
    "print(\"LSTM Model Loss:\", predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "france forcing mesa lift cleanest exonerated mine ab comeys leaking rising rancher\n"
     ]
    }
   ],
   "source": [
    "model_LSTM = my_model_LSTM(256, num_unique_words, max_sequence_length)\n",
    "model_LSTM.load('model_LSTM.h5')\n",
    "seed_text = 'france forcing'\n",
    "generated_text = model_LSTM.generate_text(seed_text, max_length=10)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1180/1180 [==============================] - 4s 3ms/step - loss: 7.4937 - accuracy: 0.1292\n",
      "LSTM Model Perplexity: 1796.618208383362\n"
     ]
    }
   ],
   "source": [
    "print(\"LSTM Model Perplexity:\", np.exp(model_LSTM.evaluate(X_test, y_test)[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We face some overfiting... The perplexity is still way better than the baseline one: 1796 compare to 3146"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4720/4720 [==============================] - 62s 13ms/step - loss: 7.2615 - accuracy: 0.0197\n",
      "Epoch 2/10\n",
      "4720/4720 [==============================] - 61s 13ms/step - loss: 6.5532 - accuracy: 0.0645\n",
      "Epoch 3/10\n",
      "4720/4720 [==============================] - 61s 13ms/step - loss: 5.9819 - accuracy: 0.1127\n",
      "Epoch 4/10\n",
      "4720/4720 [==============================] - 61s 13ms/step - loss: 5.5213 - accuracy: 0.1473\n",
      "Epoch 5/10\n",
      "4720/4720 [==============================] - 60s 13ms/step - loss: 5.1384 - accuracy: 0.1760\n",
      "Epoch 6/10\n",
      "4720/4720 [==============================] - 60s 13ms/step - loss: 4.8051 - accuracy: 0.2042\n",
      "Epoch 7/10\n",
      "4720/4720 [==============================] - 60s 13ms/step - loss: 4.5096 - accuracy: 0.2314\n",
      "Epoch 8/10\n",
      "4720/4720 [==============================] - 61s 13ms/step - loss: 4.2431 - accuracy: 0.2583\n",
      "Epoch 9/10\n",
      "4720/4720 [==============================] - 61s 13ms/step - loss: 4.0023 - accuracy: 0.2868\n",
      "Epoch 10/10\n",
      "4720/4720 [==============================] - 59s 13ms/step - loss: 3.7841 - accuracy: 0.3128\n"
     ]
    }
   ],
   "source": [
    "num_unique_words2 = len(tokenizer2.word_index) + 1\n",
    "max_sequence_length = 40\n",
    "model_LSTM2 = my_model_LSTM(100, num_unique_words2, max_sequence_length)\n",
    "model_LSTM2.train(X_train2, y_train2, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/Trump_TextGeneration_NLP_Project/myenv/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model_LSTM2.save('model_LSTM2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1180/1180 [==============================] - 5s 4ms/step - loss: 6.4061 - accuracy: 0.1577\n",
      "LSTM Model Accuracy: 0.15773722529411316\n",
      "LSTM Model Loss: 6.406148433685303\n"
     ]
    }
   ],
   "source": [
    "predictions = model_LSTM2.evaluate(X_test, y_test)\n",
    "print(\"LSTM Model Accuracy:\", predictions[1])\n",
    "print(\"LSTM Model Loss:\", predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thank legacy doesnt son helicopter loophole decide many front politicians will… twin lady mckinley rapids conducted shattering pause sharp suppliers obviously stolen rigid 94 weissman soon add surged erdogan enough theft\n"
     ]
    }
   ],
   "source": [
    "model_LSTM2 = my_model_LSTM(256, num_unique_words2, max_sequence_length)\n",
    "model_LSTM2.load('model_LSTM.h5')\n",
    "seed_text = 'thank'\n",
    "generated_text = model_LSTM2.generate_text(seed_text, max_length=30)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1180/1180 [==============================] - 4s 3ms/step - loss: 7.4937 - accuracy: 0.1292\n",
      "LSTM Model Perplexity: 1796.618208383362\n"
     ]
    }
   ],
   "source": [
    "print(\"LSTM Model Perplexity:\", np.exp(model_LSTM2.evaluate(X_test, y_test)[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With complete preprocessing, the metrics are better, but the generation is less comprehensive and realistic (no stopwords, rare words). It's a trade-off that can be interesting but not ideal for a text generator."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ncreasing the number of LSTM units takes more time to train but allows for better results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bc11ff26b6a5fa7a865d8250cd70cfc7d3342e295774ddad3442a38e57bd6f20"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
