{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSTM for Trump text generation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](https://miro.medium.com/v2/resize:fit:984/1*Mb_L_slY9rjMr8-IADHvwg.png)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LSTM stands for Long Short-Term Memory, and it's a type of RNN that is known to be effective for text generation, which is our primary goal.\n",
    "\n",
    "Advantages:\n",
    "\n",
    "- Capable of capturing complex sequential dependencies.\n",
    "- Can retain context and remember information from the entire sequence, making it ideal for contextual situations, such as discussing Trump.\n",
    "\n",
    "Limits:\n",
    "\n",
    "- LSTM models require a large amount of data; it's a complex model that demands extensive training to handle context effectively.\n",
    "- LSTMs are computationally intensive, and tuning hyperparameters has been a real challenge during this project."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tensorflow will be used for the integration of the LSTM model."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "path = 'Trump Rally Speeches/'\n",
    "files = os.listdir(path)\n",
    "files = [path + file for file in files]\n",
    " \n",
    "dates = []\n",
    "locations = []\n",
    "years = []\n",
    "days = []\n",
    "months = []\n",
    "speeches_text = []\n",
    " \n",
    "month_ab = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep','Oct', 'Nov', 'Dec']\n",
    "\n",
    "for file in files:\n",
    "    for month in month_ab:\n",
    "        if month in file:\n",
    "            locations.append(file[file.find('/')+1:file.find(month)])\n",
    "            break\n",
    "    for i, mont in enumerate(month_ab):\n",
    "        if month in file:\n",
    "            date = file[file.find(month):file.find('.txt')]\n",
    "            dates.append(date)\n",
    "            months.append(date[:3])\n",
    "            days.append(str(date[3]))\n",
    "            years.append(date[-4:])\n",
    "            break   \n",
    "        \n",
    "for file in files:\n",
    "    with open(file, 'r') as f:\n",
    "        speeches_text.append(f.read())     \n",
    "        \n",
    "import pandas as pd\n",
    " \n",
    "df = pd.DataFrame({'Speech':files, 'Date':dates, 'Location':locations, 'Year':years, 'Month':months, 'Day':days, 'Speech_Text':speeches_text})"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will use the two preprocessing function defined in preprocessing_pipline.\n",
    "One only remove the punctuation and do lower case, the other one also remove the stop and rare words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /home/codespace/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "thank thank thank vice president pence hes good guy weve done great job together merry christmas mic\n",
      "Thank you. Thank you. Thank you to Vice President Pence. He's a good guy. We've done a great job tog\n",
      "thank thank thank vice president pence hes good guy weve done great job together merry christmas mic\n"
     ]
    }
   ],
   "source": [
    "from preprocessing import preprocessing_pipline\n",
    "\n",
    "preprocessing = preprocessing_pipline(df['Speech_Text'])\n",
    "df['Speech_Text_prepro'] = preprocessing.preprocess_light()\n",
    "df['Speech_Text_prepro2'] = preprocessing.preprocess()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data \n",
    "1. Without preprocessing -> terrible results... not intresting\n",
    "2. Light preprocessing \n",
    "3. Heavy preprocessing (stop words and rare words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "text_corpus = [word for speech in df['Speech_Text'].str.split() for word in speech]\n",
    "\n",
    "# Preprocess text \n",
    "text_corpus_prepro = [word for speech in df['Speech_Text_prepro'].str.split() for word in speech]\n",
    "\n",
    "text_corpus_prepro2 = [word for speech in df['Speech_Text_prepro2'].str.split() for word in speech]\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess before using LSTM\n",
    "\n",
    "Input is the sequences and output the next word to guess.  \n",
    "-We tokenize them, and we applied a padding to both: make them the same lenght\n",
    "-Convert to numpy array  \n",
    "-Split into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-05 22:22:39.147370: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-05 22:22:39.182658: E tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:9342] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2023-11-05 22:22:39.182696: E tensorflow/compiler/xla/stream_executor/cuda/cuda_fft.cc:609] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2023-11-05 22:22:39.182736: E tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:1518] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2023-11-05 22:22:39.190621: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-11-05 22:22:39.191137: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-11-05 22:22:40.113116: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "def preprocess_for_LSTM(text_corpus, lenght_of_sequences=10, test_size=0.2):\n",
    "    tokenizer = Tokenizer()\n",
    "    tokenizer.fit_on_texts(text_corpus)\n",
    "\n",
    "    input = []\n",
    "    output = []\n",
    "\n",
    "\n",
    "    for i in range(lenght_of_sequences, len(text_corpus)):\n",
    "        input.append(text_corpus[i - lenght_of_sequences:i])\n",
    "        output.append(text_corpus[i])\n",
    "\n",
    "    input = tokenizer.texts_to_sequences(input)\n",
    "    output = tokenizer.texts_to_sequences(output)\n",
    "\n",
    "    # padding\n",
    "    input = pad_sequences(input, maxlen=lenght_of_sequences, padding='pre')\n",
    "    output = pad_sequences(output, maxlen=1, padding='pre')\n",
    "    \n",
    "    # into numpy arrays\n",
    "    input = np.array(input)\n",
    "    output = np.array(output)\n",
    "\n",
    "    # Split your data into train and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(input, output, test_size=test_size, random_state=42)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, tokenizer\n",
    "\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test, tokenizer = preprocess_for_LSTM(text_corpus_prepro, lenght_of_sequences=10, test_size=0.2)\n",
    "X_train2, X_test2, y_train2, y_test2, tokenizer2 = preprocess_for_LSTM(text_corpus_prepro2, lenght_of_sequences=10, test_size=0.2)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from keras.layers import Embedding, LSTM, Dense\n",
    "\n",
    "class my_model_LSTM:\n",
    "    def __init__(self, n, num_unique_words, max_sequence_length):\n",
    "        \"\"\"\n",
    "        Initialize the LSTM model\n",
    "        \n",
    "        Args:\n",
    "            n (int): Number of LSTM units.\n",
    "            num_unique_words (int): Number of unique words \n",
    "            max_sequence_length (int): Maximum sequence length.\n",
    "        \"\"\"\n",
    "        self.n = n\n",
    "        self.num_unique_words = num_unique_words\n",
    "        self.max_sequence_length = max_sequence_length\n",
    "        self.model = self.build_model()\n",
    "        \n",
    "    def build_model(self):\n",
    "        \"\"\"\n",
    "        Build and compile the LSTM model\n",
    "        \"\"\"\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(Embedding(self.num_unique_words, self.n))\n",
    "        model.add(LSTM(units=self.n))\n",
    "        model.add(Dense(units=self.num_unique_words, activation='softmax'))\n",
    "        model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "        return model\n",
    "     \n",
    "    def train(self, X_train, y_train, epochs=10):\n",
    "        self.model.fit(X_train, y_train, epochs=epochs)\n",
    "         \n",
    "    def predict(self, test_corpus):\n",
    "        return self.model.predict(test_corpus)\n",
    "     \n",
    "    def evaluate(self, X_test, y_test):\n",
    "        return self.model.evaluate(X_test, y_test)\n",
    "     \n",
    "    def save(self, path):\n",
    "        self.model.save(path)\n",
    "         \n",
    "    def load(self, path):\n",
    "        self.model = tf.keras.models.load_model(path)\n",
    "         \n",
    "    def generate_text(self, seed_text, max_length=100):\n",
    "        \"\"\"\n",
    "        Generate text based on a seed text\n",
    "\n",
    "        Args:\n",
    "            seed_text (str): Initial text seed, we will generate what's next\n",
    "            max_length (int): Size of the text to generate\n",
    "\n",
    "        Returns:\n",
    "            str: Generated text\n",
    "        \"\"\"\n",
    "        output_text = seed_text.split()\n",
    "        prefix = output_text[-(self.n - 1):]\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            input = [tokenizer.word_index[word] for word in prefix]\n",
    "            input = tf.expand_dims(input, 0)\n",
    "\n",
    "            predictions = self.model(input)\n",
    "            predictions = tf.squeeze(predictions, 0)\n",
    "\n",
    "            if len(predictions.shape) == 1:\n",
    "                predictions = tf.expand_dims(predictions, 0)\n",
    "\n",
    "            predicted_id = tf.random.categorical(predictions, num_samples=1)[-1, 0].numpy()\n",
    "            next_word = tokenizer.index_word[predicted_id]\n",
    "\n",
    "            output_text.append(next_word)\n",
    "            prefix = prefix[1:] + [next_word]\n",
    "\n",
    "        return ' '.join(output_text)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/40\n",
      "4720/4720 [==============================] - 39s 8ms/step - loss: 7.3394 - accuracy: 0.0138\n",
      "Epoch 2/40\n",
      "4720/4720 [==============================] - 38s 8ms/step - loss: 6.8464 - accuracy: 0.0362\n",
      "Epoch 3/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 6.4224 - accuracy: 0.0694\n",
      "Epoch 4/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 6.0620 - accuracy: 0.0997\n",
      "Epoch 5/40\n",
      "4720/4720 [==============================] - 38s 8ms/step - loss: 5.7583 - accuracy: 0.1255\n",
      "Epoch 6/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 5.5041 - accuracy: 0.1444\n",
      "Epoch 7/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 5.2868 - accuracy: 0.1613\n",
      "Epoch 8/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 5.0968 - accuracy: 0.1765\n",
      "Epoch 9/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 4.9282 - accuracy: 0.1907\n",
      "Epoch 10/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 4.7775 - accuracy: 0.2052\n",
      "Epoch 11/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 4.6424 - accuracy: 0.2178\n",
      "Epoch 12/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 4.5190 - accuracy: 0.2300\n",
      "Epoch 13/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 4.4079 - accuracy: 0.2407\n",
      "Epoch 14/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 4.3037 - accuracy: 0.2529\n",
      "Epoch 15/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 4.2096 - accuracy: 0.2637\n",
      "Epoch 16/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 4.1220 - accuracy: 0.2745\n",
      "Epoch 17/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 4.0401 - accuracy: 0.2841\n",
      "Epoch 18/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 3.9653 - accuracy: 0.2936\n",
      "Epoch 19/40\n",
      "4720/4720 [==============================] - 38s 8ms/step - loss: 3.8946 - accuracy: 0.3031\n",
      "Epoch 20/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 3.8289 - accuracy: 0.3116\n",
      "Epoch 21/40\n",
      "4720/4720 [==============================] - 38s 8ms/step - loss: 3.7669 - accuracy: 0.3193\n",
      "Epoch 22/40\n",
      "4720/4720 [==============================] - 38s 8ms/step - loss: 3.7091 - accuracy: 0.3264\n",
      "Epoch 23/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 3.6563 - accuracy: 0.3344\n",
      "Epoch 24/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 3.6051 - accuracy: 0.3416\n",
      "Epoch 25/40\n",
      "4720/4720 [==============================] - 38s 8ms/step - loss: 3.5580 - accuracy: 0.3491\n",
      "Epoch 26/40\n",
      "4720/4720 [==============================] - 38s 8ms/step - loss: 3.5114 - accuracy: 0.3552\n",
      "Epoch 27/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 3.4717 - accuracy: 0.3606\n",
      "Epoch 28/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 3.4324 - accuracy: 0.3656\n",
      "Epoch 29/40\n",
      "4720/4720 [==============================] - 38s 8ms/step - loss: 3.3942 - accuracy: 0.3709\n",
      "Epoch 30/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 3.3587 - accuracy: 0.3764\n",
      "Epoch 31/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 3.3271 - accuracy: 0.3806\n",
      "Epoch 32/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 3.2947 - accuracy: 0.3858\n",
      "Epoch 33/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 3.2656 - accuracy: 0.3908\n",
      "Epoch 34/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 3.2373 - accuracy: 0.3938\n",
      "Epoch 35/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 3.2106 - accuracy: 0.3982\n",
      "Epoch 36/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 3.1860 - accuracy: 0.4028\n",
      "Epoch 37/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 3.1599 - accuracy: 0.4071\n",
      "Epoch 38/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 3.1378 - accuracy: 0.4097\n",
      "Epoch 39/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 3.1186 - accuracy: 0.4122\n",
      "Epoch 40/40\n",
      "4720/4720 [==============================] - 37s 8ms/step - loss: 3.0941 - accuracy: 0.4162\n"
     ]
    }
   ],
   "source": [
    "num_unique_words = len(tokenizer.word_index) + 1\n",
    "max_sequence_length = 40\n",
    "model_LSTM = my_model_LSTM(50, num_unique_words, max_sequence_length)\n",
    "history = model_LSTM.train(X_train, y_train, epochs=40)\n",
    "model_LSTM.save('model_LSTM.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1180/1180 [==============================] - 4s 3ms/step - loss: 7.4937 - accuracy: 0.1292\n",
      "LSTM Model Accuracy: 0.12921422719955444\n",
      "LSTM Model Loss: 7.493661403656006\n"
     ]
    }
   ],
   "source": [
    "predictions = model_LSTM.evaluate(X_test, y_test)\n",
    "print(\"LSTM Model Accuracy:\", predictions[1])\n",
    "print(\"LSTM Model Loss:\", predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "france forcing mesa lift cleanest exonerated mine ab comeys leaking rising rancher\n"
     ]
    }
   ],
   "source": [
    "model_LSTM = my_model_LSTM(256, num_unique_words, max_sequence_length)\n",
    "model_LSTM.load('model_LSTM.h5')\n",
    "seed_text = 'france forcing'\n",
    "generated_text = model_LSTM.generate_text(seed_text, max_length=10)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1180/1180 [==============================] - 4s 3ms/step - loss: 7.4937 - accuracy: 0.1292\n",
      "LSTM Model Perplexity: 1796.618208383362\n"
     ]
    }
   ],
   "source": [
    "print(\"LSTM Model Perplexity:\", np.exp(model_LSTM.evaluate(X_test, y_test)[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We face some overfiting... The perplexity is still way better than the baseline one: 1796 compare to 3146"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Light preprocessing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "4720/4720 [==============================] - 62s 13ms/step - loss: 7.2615 - accuracy: 0.0197\n",
      "Epoch 2/10\n",
      "4720/4720 [==============================] - 61s 13ms/step - loss: 6.5532 - accuracy: 0.0645\n",
      "Epoch 3/10\n",
      "4720/4720 [==============================] - 61s 13ms/step - loss: 5.9819 - accuracy: 0.1127\n",
      "Epoch 4/10\n",
      "4720/4720 [==============================] - 61s 13ms/step - loss: 5.5213 - accuracy: 0.1473\n",
      "Epoch 5/10\n",
      "4720/4720 [==============================] - 60s 13ms/step - loss: 5.1384 - accuracy: 0.1760\n",
      "Epoch 6/10\n",
      "4720/4720 [==============================] - 60s 13ms/step - loss: 4.8051 - accuracy: 0.2042\n",
      "Epoch 7/10\n",
      "4720/4720 [==============================] - 60s 13ms/step - loss: 4.5096 - accuracy: 0.2314\n",
      "Epoch 8/10\n",
      "4720/4720 [==============================] - 61s 13ms/step - loss: 4.2431 - accuracy: 0.2583\n",
      "Epoch 9/10\n",
      "4720/4720 [==============================] - 61s 13ms/step - loss: 4.0023 - accuracy: 0.2868\n",
      "Epoch 10/10\n",
      "4720/4720 [==============================] - 59s 13ms/step - loss: 3.7841 - accuracy: 0.3128\n"
     ]
    }
   ],
   "source": [
    "num_unique_words2 = len(tokenizer2.word_index) + 1\n",
    "max_sequence_length = 40\n",
    "model_LSTM2 = my_model_LSTM(100, num_unique_words2, max_sequence_length)\n",
    "model_LSTM2.train(X_train2, y_train2, epochs=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspaces/Trump_TextGeneration_NLP_Project/myenv/lib/python3.10/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    }
   ],
   "source": [
    "model_LSTM2.save('model_LSTM2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1180/1180 [==============================] - 5s 4ms/step - loss: 6.4061 - accuracy: 0.1577\n",
      "LSTM Model Accuracy: 0.15773722529411316\n",
      "LSTM Model Loss: 6.406148433685303\n"
     ]
    }
   ],
   "source": [
    "predictions = model_LSTM2.evaluate(X_test, y_test)\n",
    "print(\"LSTM Model Accuracy:\", predictions[1])\n",
    "print(\"LSTM Model Loss:\", predictions[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thank legacy doesnt son helicopter loophole decide many front politicians willâ€¦ twin lady mckinley rapids conducted shattering pause sharp suppliers obviously stolen rigid 94 weissman soon add surged erdogan enough theft\n"
     ]
    }
   ],
   "source": [
    "model_LSTM2 = my_model_LSTM(256, num_unique_words2, max_sequence_length)\n",
    "model_LSTM2.load('model_LSTM.h5')\n",
    "seed_text = 'thank'\n",
    "generated_text = model_LSTM2.generate_text(seed_text, max_length=30)\n",
    "print(generated_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1180/1180 [==============================] - 4s 3ms/step - loss: 7.4937 - accuracy: 0.1292\n",
      "LSTM Model Perplexity: 1796.618208383362\n"
     ]
    }
   ],
   "source": [
    "print(\"LSTM Model Perplexity:\", np.exp(model_LSTM2.evaluate(X_test, y_test)[0]))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With complete preprocessing, the metrics are better, but the generation is less comprehensive and realistic (no stopwords, rare words). It's a trade-off that can be interesting but not ideal for a text generator."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ncreasing the number of LSTM units takes more time to train but allows for better results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "bc11ff26b6a5fa7a865d8250cd70cfc7d3342e295774ddad3442a38e57bd6f20"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
